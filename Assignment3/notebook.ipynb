{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Afeka - ML3 - Titanic\n",
    "\n",
    "Noam Levi  \n",
    "205530611  \n",
    "[Kaggle Profile](https://www.kaggle.com/noamlevi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to be working on the [Titanic dataset](https://www.kaggle.com/c/titanic/data) from the [kaggle competition](https://www.kaggle.com/c/titanic).  \n",
    "\n",
    "This is a continued work from `Assignment1`.\n",
    "\n",
    "---\n",
    "\n",
    "Roadmap:\n",
    "- [Data Exploration](#Data-Exploration) and Data Visualising - *from `Assignment1`*\n",
    "- [Data Cleaning](#Data-Cleaning), handling missing data in our df using different methods - *from `Assignment1`*\n",
    "- [Feature Engineering](#Feature-Engineering), creating/choosing the right features for a better ML model - *from `Assignment1`*\n",
    "- [Training & Model Comparing](#Training-&-Model-Comparing), Fitting a linear model & loss function plotting using different hyperparameters\n",
    "- [Testing](#Testing)\n",
    "- [Summary](#Summary)\n",
    "- [References](#References)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "# from pandas_profiling import ProfileReport\n",
    "import sweetviz as sv\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import seaborn as sns\n",
    "#from scipy import stats\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer #, StandardScaler, Normalizer, LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import LeavePOut, KFold, GridSearchCV #, cross_validate, train_test_split\n",
    "# from sklearn.feature_selection import RFE\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB as GaussianNBC\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import GradientBoostingClassifier, BaggingClassifier\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "# plt.style.use(\"fivethirtyeight\")"
   ]
  },
  {
   "source": [
    "Joining `train` & `test` to a single `data` df for an easier time while working on the model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_col = 'PassengerId'\n",
    "train = pd.read_csv('../data/train.csv', index_col=index_col)\n",
    "test = pd.read_csv('../data/test.csv', index_col=index_col)\n",
    "\n",
    "ntrain = train.shape[0]\n",
    "ntest = test.shape[0]\n",
    "data = pd.concat([train, test]).reset_index(drop=True)\n",
    "# data\n",
    "\n",
    "# train = pd.get_dummies(data)[:ntrain]\n",
    "# test = pd.get_dummies(data)[:ntest]\n",
    "# test.index = itest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global pipeline\n",
    "pipeline = Pipeline([(\n",
    "    'none',\n",
    "    FunctionTransformer(func=None)\n",
    ")])\n",
    "# use pipeline.steps.append() for adding steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funcs = [(func1, name1), func2, func3, ...]\n",
    "\n",
    "def appendToPipeline(pipeline, funcs):\n",
    "    names = []\n",
    "    for (n,f) in pipeline.steps:\n",
    "        names += [n]\n",
    "\n",
    "    for item in funcs:\n",
    "        func = None\n",
    "        name = None\n",
    "        if isinstance(item, (tuple, list)):\n",
    "            func = item[0]\n",
    "            name = item[1]\n",
    "        else:\n",
    "            func = item\n",
    "        \n",
    "        if name == None:\n",
    "            name = func.__name__\n",
    "                \n",
    "        if name not in names:\n",
    "            pipeline.steps.append((\n",
    "                name,\n",
    "                FunctionTransformer(func)\n",
    "            ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start thing off, we can use ~~pandas_profiling~~ sweetviz library to get an overview of the entire training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = sv.analyze(train, target_feat='Survived')\n",
    "report.show_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "\n",
    "display(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the target value's distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "survived_per_sex = pd.DataFrame()\n",
    "survived_per_sex['Sex'] = ['female','male']*2\n",
    "survived_per_sex['Survived'] = ['Survived']*2 + ['Died']*2\n",
    "survived_per_sex['Amount'] = (\n",
    "    list(train[train['Survived']==1].groupby('Sex').agg('count')['Survived']) +\n",
    "    list(train[train['Survived']==0].groupby('Sex').agg('count')['Survived'])\n",
    ")\n",
    "\n",
    "# survived_per_sex\n",
    "plt.subplots(figsize=(10,6))\n",
    "ax = sns.barplot(x='Sex', y='Amount', hue='Survived', order=['male', 'female'], data=survived_per_sex)\n",
    "ax.set_title('Survivors by Sex', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the complete data exploration, see `Assignment1`'s notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "source": [
    "We will copy the cleaning step from `Assignment1`'s notebook."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def NAs(data):\n",
    "data_na = (data.drop(columns=['Survived']).isnull().sum() / len(data)) * 100\n",
    "# data_na = (data_dummy.drop(columns=['Survived']).isnull().sum() / len(data)) * 100\n",
    "data_na = data_na.drop(data_na[data_na == 0].index).sort_values(ascending=False)[:30]\n",
    "missing_data = pd.DataFrame({'Missing %': data_na})\n",
    "missing_data.head()\n",
    "# return missing_data\n",
    "\n",
    "# NAs(train).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_cabin(df):\n",
    "    df = df.copy()\n",
    "    df['Cabin'].fillna('X', inplace=True)\n",
    "    return data\n",
    "\n",
    "def fix_na(df):\n",
    "    df = df.copy()\n",
    "    df['Age'].fillna(df['Age'].median(), inplace=True)\n",
    "    df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)\n",
    "    df['Fare'].fillna(df['Fare'].median(), inplace=True) # for the test data, has missing values.\n",
    "    return data\n",
    "\n",
    "appendToPipeline(pipeline, [handle_cabin, fix_na])\n",
    "data_filled = pipeline.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_filled.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "source": [
    "We will copy the feature engineering step from `Assignment1`'s notebook."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_cat(df):\n",
    "    df = df.copy()\n",
    "    cols = ['Embarked','Sex', 'Pclass']\n",
    "    for col in cols:\n",
    "        df[col] = df[col].astype('category').cat.codes.astype('category')\n",
    "    return df\n",
    "\n",
    "def bin_cols(df):\n",
    "    df = df.copy()\n",
    "    df['Age'] = pd.cut(df['Age'], 10).cat.codes.astype('category')\n",
    "    df['Fare'] = pd.qcut(df['Fare'], 13, duplicates='drop').cat.codes.astype('category')\n",
    "    return df\n",
    "\n",
    "def add_title_col(df):\n",
    "    df = df.copy()\n",
    "    global titles\n",
    "    df['Title'] = \\\n",
    "        df['Name'].str \\\n",
    "        .split(', ', expand=True)[1].str \\\n",
    "        .split(' ', expand=True)[0] \\\n",
    "        .apply(lambda val: val.strip('.'))\n",
    "    \n",
    "    df['Title'] = df['Title'].replace('Mlle', 'Miss')\n",
    "    df['Title'] = df['Title'].replace('Ms', 'Miss')\n",
    "    df['Title'] = df['Title'].replace('Mme', 'Mrs')\n",
    "    # df['Title'] = df['Title'].replace('Mrs', 'Miss') # keep miss instead of mrs\n",
    "    \n",
    "    titles = list(df['Title'].value_counts().where(df['Title'].value_counts() > 10).dropna().index)\n",
    "    df['Title'] = df['Title'].apply(lambda title: title if title in titles else 'Else').astype('category')\n",
    "    titles = list(df['Title'].cat.categories) # keep track of all titles in the df for later use\n",
    "\n",
    "    # Encoding `Title` as categorial int\n",
    "    df['Title'] = df['Title'].astype('category').cat.codes.astype('category')\n",
    "    df.drop(columns='Name', inplace=True)\n",
    "    return df\n",
    "\n",
    "def add_deck_col(df):\n",
    "    df = df.copy()\n",
    "    global decks\n",
    "    df['Deck'] = df['Cabin'].apply( lambda val: str(val)[0].upper() )\n",
    "    df.drop(columns=['Cabin'], inplace=True)\n",
    "    \n",
    "    # check is `decks` already defined\n",
    "    try:\n",
    "        decks\n",
    "    except NameError:\n",
    "        decks = list(train_df['Deck'].astype('category').cat.categories)\n",
    "    finally:\n",
    "        df['Deck'] = df['Deck'].astype('category').cat.codes.astype('category')\n",
    "        return df\n",
    "\n",
    "def handle_ticket(df):\n",
    "    df = df.copy()\n",
    "    df['Ticket_Frequency'] = df.groupby('Ticket')['Ticket'].transform('count')\n",
    "    df.drop(columns=['Ticket'], inplace=True)\n",
    "    return df\n",
    "\n",
    "def add_family_col(df):\n",
    "    def family_map(size):\n",
    "        if size < 2:\n",
    "            return 'Alone'\n",
    "        elif size <= 4:\n",
    "            return 'Small'\n",
    "        elif size <= 6:\n",
    "            return 'Medium'\n",
    "        else: # size > 6\n",
    "            return 'Large'\n",
    "\n",
    "    df = df.copy()\n",
    "    df['FamilySize'] = 1 + df['SibSp'] + df['Parch']\n",
    "    df['FamilySize'] = df['FamilySize'].apply(family_map)\n",
    "    df['FamilySize'] = df['FamilySize'].astype('category').cat.codes.astype('category')\n",
    "    df.drop(columns=['SibSp', 'Parch'], inplace=True)\n",
    "    return df\n",
    "\n",
    "def get_dummies(df):\n",
    "    df = df.copy()\n",
    "    return pd.get_dummies(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appendToPipeline(\n",
    "    pipeline,\n",
    "    [\n",
    "        str_to_cat,\n",
    "        bin_cols,\n",
    "        add_title_col,\n",
    "        add_deck_col,\n",
    "        handle_ticket,\n",
    "        add_family_col,\n",
    "        get_dummies\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dummy = pipeline.fit_transform(data.copy())\n",
    "data_dummy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dummy.info()"
   ]
  },
  {
   "source": [
    "Now we can split our `data` back into `train` and `test`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dummy = pd.get_dummies(data_filled)[:ntrain]\n",
    "# test_dummy = pd.get_dummies(data_filled)[:ntest]\n",
    "# test_dummy.index = test.index\n",
    "\n",
    "# train_no_dummy = data_filled[:ntrain]\n",
    "# test_no_dummy = data_filled[:ntest]\n",
    "\n",
    "data_dummy = pipeline.fit_transform(data)\n",
    "train_dummy = data_dummy[:ntrain]\n",
    "test_dummy = data_dummy[ntrain:].drop(columns=['Survived'])\n",
    "test_dummy.index = test.index\n",
    "\n",
    "display(train_dummy.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dummy.info()"
   ]
  },
  {
   "source": [
    "## Training & Model Comparing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "As hyper-paramater testing we will choose different sized feature sets.  \n",
    "\n",
    "That way we could control the polynomial degree of our model, which in turn will affect the model's flexibility."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_k(start, step, length):\n",
    "    lst = []\n",
    "    for i in range(length):\n",
    "        lst += [start + (step*i)]\n",
    "    return lst\n",
    "    \n",
    "    # def inner(start, step, size):\n",
    "    #     curr = start\n",
    "    #     while size != 0:\n",
    "    #         yield curr\n",
    "    #         curr += step\n",
    "    #         size -= 1\n",
    "    # return [i for i in inner(start, step, size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = gen_k(start=5, step=4, length=13)\n",
    "# k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SalePrice correlation matrix\n",
    "train_dummy = pipeline.fit_transform(train)\n",
    "corr = train_dummy.corr()\n",
    "feats = []\n",
    "\n",
    "# k = number of diffrent feature sets to choose (polynomial degree)\n",
    "k = gen_k(start=5, step=4, length=13) # => [5, 9, ..., 53]\n",
    "\n",
    "# picking the top correlated features, not including the target\n",
    "for n in k:\n",
    "    # cols = np.abs(corr).nlargest(n+1, 'SalePrice')['SalePrice'].index.tolist()\n",
    "    cols = corr.nlargest(n+1, 'Survived')['Survived'].index.tolist()\n",
    "    cols.remove('Survived')\n",
    "    feats += [cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats[4]"
   ]
  },
  {
   "source": [
    "~~Let's try the sklearn backwards feature selection method, `RFE`.~~\n",
    "\n",
    "The feature selection based on correletion to the target worked much better than `RFE`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # est = SGDRegressor(learning_rate='optimal')\n",
    "# # est = SGDRegressor()\n",
    "# est = LinearRegression(normalize=True)\n",
    "\n",
    "# data = pipeline.fit_transform(train)\n",
    "# y = data['SalePrice']\n",
    "# X = data.drop(columns=['SalePrice'])\n",
    "# feats = []\n",
    "\n",
    "# # k = number of diffrent feature sets to choose (polynomial degree)\n",
    "# k = gen_k(start=10, step=7, length=4) # => [10, 17, 24, 31, 38, 45, 52, 59, 66, 73]\n",
    "\n",
    "# for n in tqdm(k):\n",
    "#     rfe = RFE(\n",
    "#         estimator = est,\n",
    "#         n_features_to_select = n\n",
    "#     )\n",
    "#     rfe.fit_transform(X, y)\n",
    "#     # feats += [ X.columns[rfe.support_].tolist() ]\n",
    "#     feats += [ X.columns[rfe.support_ == False].tolist() ]\n",
    "\n",
    "# print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feats[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot one of the different heatmaps we chose in the last cell\n",
    "plt.figure(figsize=(24,11))\n",
    "sns.set(font_scale=1.25)\n",
    "\n",
    "# i = random.choice(range(len(feats)))\n",
    "\n",
    "cols = ['Survived'] + feats[4]\n",
    "# cm = np.corrcoef(train_dummy[cols].values.T)\n",
    "hm = sns.heatmap(\n",
    "    train_dummy[cols].corr(),\n",
    "    cbar = True,\n",
    "    annot = True,\n",
    "    square = True,\n",
    "    fmt = '.2f',\n",
    "    cmap = 'coolwarm',\n",
    "    yticklabels = cols,\n",
    "    xticklabels = cols,\n",
    "    # title = f'Top {len(feats)-1} Correlated Features',\n",
    "    annot_kws = {'size': 11}\n",
    ")\n",
    "\n",
    "hm.set(title=f'Top {len(cols)-1} Correlated Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lrmse(y_true, y_pred, squared):\n",
    "    return metrics.mean_squared_error(np.log(y_true), np.log(y_pred), squared=squared)\n",
    "\n",
    "lrmse_socrer = make_scorer(lrmse, greater_is_better=False, squared=True)\n",
    "lmse_socrer = make_scorer(lrmse, greater_is_better=False, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ESTIMATORS ###\n",
    "estimators = []\n",
    "# estimators += [SGDRegressor(learning_rate='optimal')]\n",
    "# estimators += [SGDRegressor()]\n",
    "# estimators += [LinearRegression(normalize=True)]\n",
    "# estimators += [{'model':Ridge, 'kws':{'normalize':True, 'alpha':0.5}}]\n",
    "estimators += [{'model':Ridge, 'kws':{'normalize':True, 'alpha':0.29}}]\n",
    "estimators += [{'model':Ridge, 'kws':{'normalize':True, 'alpha':0.2}}]\n",
    "# estimators += [{'model':Ridge, 'kws':{'normalize':True, 'alpha':0.05}}]\n",
    "# estimators += [ElasticNet(normalize=True, alpha=0.5)]\n",
    "# estimators += [Lasso(normalize=True, alpha=0.5)]\n",
    "# estimators += [Lasso(normalize=True, alpha=0.2)]\n",
    "### ESTIMATORS ###\n",
    "\n",
    "### CV METHODS ###\n",
    "# l1o = LeavePOut(p=1)\n",
    "# fivefold = KFold(n_splits=5, shuffle=True, random_state=101)\n",
    "tenfold = KFold(n_splits=10, shuffle=True, random_state=101)\n",
    "### CV METHODS ###\n",
    "\n",
    "scores = {}\n",
    "# scores = dict()\n",
    "\n",
    "for d in estimators:\n",
    "    model = d['model']\n",
    "    kws = d['kws']\n",
    "    est = model(**kws)\n",
    "    d['est'] = est\n",
    "\n",
    "    est_name = \"\"\n",
    "    for i in str(est.__repr__).strip(\"<>'\").split(' ')[-2:]:\n",
    "        est_name += i.strip('of ')\n",
    "    scores[est_name] = []\n",
    "\n",
    "    for cols in tqdm(feats):\n",
    "        data = pipeline.fit_transform(train)\n",
    "        X = data[cols]\n",
    "        y = data['SalePrice']\n",
    "        res = cross_validate(\n",
    "            X = X,\n",
    "            y = y,\n",
    "            estimator = est,\n",
    "            cv = tenfold.split(X),\n",
    "            # cv = fivefold.split(X),\n",
    "            # cv = l1o.split(X),\n",
    "            return_train_score = True,\n",
    "            return_estimator = True,\n",
    "            scoring = {\n",
    "                'neg_LRMSE': lrmse_socrer,\n",
    "                'neg_LMSE': lmse_socrer,\n",
    "                'neg_MSE': 'neg_mean_squared_error',\n",
    "                # 'neg_MAE': 'neg_mean_absolute_error',\n",
    "                # 'neg_MSLE': 'neg_mean_squared_log_error'\n",
    "            }\n",
    "        )\n",
    "        res['feats'] = cols\n",
    "        scores[est_name] += [res]\n",
    "\n",
    "print('\\n\\ndone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est_names = list(scores.keys())\n",
    "print('estimators:\\n', est_names)\n",
    "print('\\nscores:\\n', list(scores[est_names[0]][0].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defs for easier access to loss function\n",
    "# MSE = 'neg_mean_squared_error'\n",
    "# LRMSE = 'neg_log_root_mean_squared_error'\n",
    "# MAE = 'neg_mean_absolute_error'\n",
    "# MSLE = 'neg_mean_squared_log_error'\n",
    "\n",
    "MSE = 'MSE'\n",
    "LRMSE = 'LRMSE'\n",
    "LMSE = 'LMSE'\n",
    "\n",
    "# losses = [MSE, RMSE, MAE, MSLE]\n",
    "# losses = [MSE, LRMSE, MAE, MSLE]\n",
    "losses = [MSE, LRMSE, LMSE]"
   ]
  },
  {
   "source": [
    "Now we're going to need to choose the best estimator among the estimators we tried.  \n",
    "\n",
    "Let's go over them in a clean df, each score in this df is the mean of all scores that estimator got."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cols = [f'test_{loss}' for loss in losses]\n",
    "estimators_df = pd.DataFrame(columns=cols, index=est_names)\n",
    "\n",
    "for est in est_names:\n",
    "    s = scores[est]\n",
    "    scores_df = pd.DataFrame(s)\n",
    "\n",
    "    for loss in losses:\n",
    "        estimators_df[f'test_{loss}'][est] = np.mean(-scores_df[f'test_neg_{loss}'].apply(np.mean))\n",
    "\n",
    "estimators_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll choose the best estimator\n",
    "\n",
    "# loss = [MSE, 'MSE']\n",
    "# loss = [LRMSE, 'LRMSE']\n",
    "loss = [LMSE, 'LMSE']\n",
    "\n",
    "best_score = np.min(estimators_df[f'test_{loss[0]}'])\n",
    "best_est = estimators_df[ estimators_df[f'test_{loss[0]}']==best_score ].index[0]\n",
    "\n",
    "print('the best estimator is:\\n', best_est)"
   ]
  },
  {
   "source": [
    "Let's choose one of the models and take a look of what we got so far.  \n",
    "\n",
    "We could plot the difference between the predictions and the true value of the target.  \n",
    "In a perfect world we would get a 45deg function ($f_{(x)}=x$ âžœ $y_{pred}=y_{true}$)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "def plot_predictions(data, feats, model, title=None, axis=None):\n",
    "    if title == None:\n",
    "        title = f'deg={len(feats)}'\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'y_true': data['SalePrice'],\n",
    "        'y_pred': model.predict(data[feats])\n",
    "    })\n",
    "    g = sns.regplot(\n",
    "        x = 'y_true',\n",
    "        y = 'y_pred',\n",
    "        data = df,\n",
    "        line_kws = {'color': '#B55D60'},\n",
    "        scatter_kws = {'edgecolor': 'white'},\n",
    "        ax = axis\n",
    "    )\n",
    "    g.set(title = title)\n",
    "    return g\n",
    "\n",
    "s = random.choice(scores[best_est])\n",
    "cols = s['feats']\n",
    "model = random.choice(s['estimator'])\n",
    "train_dummy = pipeline.fit_transform(train)\n",
    "\n",
    "plot_predictions(train_dummy, cols, model, title=f'Estimator = {best_est}\\nPoly_Degree = {len(cols)}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=math.ceil(len(k)/3), ncols=3, sharex=True, sharey=True) ##sharex=True, sharey=True##\n",
    "fig.set_size_inches(17, 22)\n",
    "# plt.figure(figsize=(16,16))\n",
    "fig.text(0.5, 0.04, 'y_true', ha='center')\n",
    "fig.text(0.04, 0.5, 'y_pred', va='center', rotation='vertical')\n",
    "axs = axs.flatten().tolist()\n",
    "\n",
    "train_dummy = pipeline.fit_transform(train)\n",
    "\n",
    "for s in tqdm(scores[best_est]):\n",
    "    cols = s['feats']\n",
    "    model = random.choice(s['estimator'])\n",
    "\n",
    "    g = plot_predictions(train_dummy, cols, model, title=f'Estimator = {best_est}\\nPoly_Degree = {len(cols)}', axis=axs.pop(0))\n",
    "    g.set(xlabel='', ylabel='')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df = pd.DataFrame(scores[best_est]).drop(columns=['fit_time', 'score_time', 'estimator'])\n",
    "\n",
    "for loss in losses:\n",
    "    scores_df[f'test_{loss}'] = -scores_df[f'test_neg_{loss}'].apply(np.mean)\n",
    "    scores_df[f'train_{loss}'] = -scores_df[f'train_neg_{loss}'].apply(np.mean)\n",
    "    scores_df = scores_df.drop(columns=[f'test_neg_{loss}', f'train_neg_{loss}'])\n",
    "\n",
    "# scores_df[f'test_{RMSE}'] = np.sqrt(scores_df[f'test_{MSE}'])\n",
    "# scores_df[f'train_{RMSE}'] = np.sqrt(scores_df[f'train_{MSE}'])\n",
    "\n",
    "scores_df['deg'] = scores_df['feats'].apply(len)\n",
    "\n",
    "scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(loss, x, y, data=scores_df):\n",
    "    sns.lineplot(\n",
    "        x = 'deg',\n",
    "        y = f'{y}_{loss[0]}',\n",
    "        data = scores_df,\n",
    "        # data = np.log(scores_df[['deg', f'{y}_{loss[0]}']]),\n",
    "        marker = 'o'\n",
    "    )\n",
    "\n",
    "    # plt.title('Err Plot')\n",
    "    plt.xlabel('Degree of Polynomial')\n",
    "    plt.ylabel(f'Loss = {loss[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "# loss = [MSE, 'MSE']\n",
    "# loss = [LRMSE, 'LRMSE']\n",
    "loss = [LMSE, 'LMSE']\n",
    "# loss = [MAE, 'MAE']\n",
    "# loss = [MSLE, 'MSLE']\n",
    "\n",
    "plot_loss(\n",
    "    loss,\n",
    "    x = 'deg',\n",
    "    y = 'train',\n",
    "    data = scores_df\n",
    ")\n",
    "\n",
    "plot_loss(\n",
    "    loss,\n",
    "    x = 'deg',\n",
    "    y = 'test',\n",
    "    data = scores_df\n",
    ")\n",
    "\n",
    "plt.title(f'Estimator = {best_est}')\n",
    "plt.legend(['train', 'test'], fontsize='large')\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "## Testing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "By looking at the graph above we can notice that the best model we fitted had around 80 features.  \n",
    "\n",
    "Let's choose that model and try and predict the actual test data from kaggle."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = MSE\n",
    "loss = LMSE\n",
    "\n",
    "# get the best score's feature list\n",
    "best_score = min(scores_df[f'test_{loss}'])\n",
    "i = scores_df[scores_df[f'test_{loss}'] == best_score].index.tolist()[0]\n",
    "cols = scores[best_est][i]['feats']\n",
    "\n",
    "# get a clean train & test DFs\n",
    "train_dummy = pipeline.fit_transform(train)\n",
    "test_dummy = pipeline.fit_transform(test)\n",
    "\n",
    "# train the model on the entire train set\n",
    "# for i,est in enumerate(estimators):\n",
    "for d in estimators:\n",
    "    est = d['est']\n",
    "    if best_est in str(est.__repr__):\n",
    "        break\n",
    "# est = estimators[i]\n",
    "model = est.fit(train_dummy[cols], train_dummy['SalePrice'])\n",
    "\n",
    "# predict the test set\n",
    "y_pred = model.predict(test_dummy[cols])\n",
    "\n",
    "pred = pd.DataFrame(\n",
    "    y_pred,\n",
    "    columns = ['SalePrice'],\n",
    "    index = test.index\n",
    ")\n",
    "\n",
    "display(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the fitted parameters used by the function\n",
    "plt.figure(figsize=(16,8))\n",
    "\n",
    "(mu, sigma) = ( round(item, 2) for item in stats.norm.fit(pred) )\n",
    "\n",
    "display(Markdown(f'$\\mu$ = {mu}'))\n",
    "display(Markdown(f'$\\sigma$ = {sigma}'))\n",
    "\n",
    "# plot the distribution\n",
    "sns.distplot(\n",
    "    pred,\n",
    "    kde_kws = {'color': '#4C4C4C'}\n",
    ")\n",
    "plt.legend(\n",
    "    [f'y_pred ~ $N(\\mu=${mu}, $\\sigma=${sigma}$)$'],\n",
    "    fontsize = 'x-large'\n",
    ")\n",
    "# plt.ylabel('Frequency')\n",
    "plt.title('Prediction Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # loss = MSE\n",
    "# loss = LMSE\n",
    "\n",
    "# # get the best score's feature list\n",
    "# best_score = min(scores_df[f'test_{loss}'])\n",
    "# i = scores_df[scores_df[f'test_{loss}'] == best_score].index.tolist()[0]\n",
    "# cols = scores[best_est][i]['feats']\n",
    "\n",
    "# # get a clean train & test DFs\n",
    "# # train_dummy = pipeline.fit_transform(train)\n",
    "# test_dummy = pipeline.fit_transform(test)\n",
    "\n",
    "# # train the model on the entire train set\n",
    "# # for i,est in enumerate(estimators):\n",
    "# # for est in estimators:\n",
    "# #     if best_est in str(est.__repr__):\n",
    "# #         break\n",
    "# # est = estimators[i]\n",
    "# # model = est.fit(train_dummy[cols], train_dummy['SalePrice'])\n",
    "\n",
    "# # scores[best_est][i]['estimator']\n",
    "\n",
    "# # predict the test set\n",
    "# preds = pd.DataFrame(columns=[i for i in range(len(scores[best_est][i]['estimator']))])\n",
    "# for i,model in  enumerate(scores[best_est][i]['estimator']):\n",
    "#     preds[i] = model.predict(test_dummy[cols])\n",
    "# preds\n",
    "\n",
    "# y_pred = []\n",
    "# for i in preds.index:\n",
    "#     y_pred += [np.mean(preds.iloc[i])]\n",
    "\n",
    "# pred = pd.DataFrame(\n",
    "#     y_pred,\n",
    "#     columns = ['SalePrice'],\n",
    "#     index = test.index\n",
    "# )\n",
    "\n",
    "# display(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.to_csv('pred.csv')"
   ]
  },
  {
   "source": [
    "## Summary"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Screenshots"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "![submissions](./screenshots/submissions.png)  \n",
    "\n",
    "![leaderboards](./screenshots/leaderboards.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Conclusions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.8 64-bit ('.env')",
   "metadata": {
    "interpreter": {
     "hash": "a0fb33b753a64e5a553c059f2c84126fb2ec11a946148c8565f5d230e918747d"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}